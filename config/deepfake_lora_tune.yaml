"tuning_type": "LoraFT"
"model_name": "deepfake_detection"
"real_folder": ['Real_5_k_split']
"fake_folder": ['StableDiffusion_split', 'StyleGAN2_split', 'Midjourney_split', 'firefly_split', 'Dall-E_split']
"test_split": [Test]
"num_epochs": 10
"batch_size": 16
"learning_rate": 0.0005
"use_wandb": True
"experiment_name": "experiment1"

# Lora
"r": 8
"lora_alpha": 16
"target_modules": ["self_attn.q_proj", "self_attn.v_proj"]
"lora_dropout": 0.05
"bias": "none"